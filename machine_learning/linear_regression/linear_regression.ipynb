{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Introduction\n",
    "Is an algorithm for fitting linear model. Given linear model described by equation $ y = wx + b $, it finds the values for $w$ and $b$ such that the model \"fits\" the provided data. Meaning, given input values $x$ it predicts as acurately as possible the values of $y$.<br>\n",
    "<br>\n",
    "For demonstration, look at the plot below. The blue dots are data points and the red line represents a model that has been fit to that data - it's $w$ and $b$ parameters are such that given linear equation $ y = wx + b $ it correctly predicts $y$ for given input values $x$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear model plot](linear_model_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model from above example perfectly matches the data points. In real examples, however this will happen very rarely. In fact, the data points will almost always be such, that it is impossible to find parameters that put them all on the same line. See the example below for demonstration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear model plot - random data points](linear_model_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can clearly see there is no straight, red line that we could draw, that would contain all the blue dots. How, then can we fit linear model's parameters to such data? By doing the best we can in that situation, which means finding parameters $w$ and $b$ to minimize for each $x$, the distance between model's prediction (the red line) - $\\hat{y}$ and actual value for that data point - $y$. <br>\n",
    "<br>\n",
    "In other words we will calculate, and try to minimize, the error, also called the **cost function**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the error\n",
    "\n",
    "The function that defines the error **for single data point** is called the **loss function**.<br>\n",
    "<br>\n",
    "For linear regression, the error value for each **individual** data point will be described as a **squarred** distance between model's prediction and actual value for that data point, found in the dataset. Mathematically, it is represented as:\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "\\begin{equation}\n",
    "l(y^{(j)}, \\hat{y}^{(j)}) = (y^{(j)} - \\hat{y}^{(j)})^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where:<br><br>\n",
    "$j$ - index of a data point in a dataset.<br>\n",
    "$y^{(j)}$ - value for j-th data point in the dataset.<br>\n",
    "$\\hat{y}^{(j)}$ - value predicted by the model, for j-th data point in the dataset.<br>\n",
    "\n",
    "However, to calculate the error that can be used to fit the model, we need to know error for the whole dataset. To measure that we will take a mean of loss function values over all data points.<br>\n",
    "This is called **cost function** and can be represented by equation (please don't confuse cost function symbol $J$ with $J$ used to represent matrix of ones in linear model notebook):\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "\\begin{equation}\n",
    "J(\\hat{y},y) = {1 \\over m}\\displaystyle\\sum_{j=0}^m{(\\hat{y}^{(j)} - y^{(j)})^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where:<br><br>\n",
    "$m$ - number of data points in the dataset.<br>\n",
    "<br><br>\n",
    "It is worth to note that in most machine learning materials this cost function contains the factor of 2 in the denominator. The reason for that constant to be there is to simplify the equation after taking it's derivative (more on that later). The equation then is:\n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "\\begin{equation}\n",
    "J(\\hat{y}, y) = {1 \\over 2m}\\displaystyle\\sum_{j=0}^m{(\\hat{y}^{(j)} - y^{(j)})^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "<br><br>\n",
    "The value predicted by the model $\\hat{y}$ represents the linear model: $\\hat{y} = wx + b$, so equation $(3)$ can be finally presented as:\n",
    "\n",
    "$$\n",
    "\\tag{4}\n",
    "\\begin{equation}\n",
    "J(\\hat{y}, y) = J(\\vec{x}, b, y) = {1 \\over 2m}\\displaystyle\\sum_{j=0}^m{(\\vec{x}^{(j)}\\vec{w} + b - y^{(j)})^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where:<br><br>\n",
    "$w$ - input variable coefficient, weights.<br>\n",
    "$x^{(j)}$ - vector of independend variables for j-th data point.<br>\n",
    "$b$ - bias coefficient.<br>\n",
    "\n",
    "## Using Gradient Descent to fit parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to the data means we will find model parameters $\\vec{w}$ and $b$ for which $J(\\vec{w}, b, y)$ is minimal. For convenience let's define model parameters as single variable theta: $\\theta = \\{\\vec{w}, b\\}$. Our task then is to find $\\theta_{opt}$ as:\n",
    "$$\n",
    "\\tag{5}\n",
    "\\theta_{opt} = \\argmin_{\\theta} J(\\theta, y)\n",
    "$$\n",
    "<br><br>\n",
    "In order to find optimal parameters we will use a procedure called Gradient Descent. This algorithm requires us to iteratively update model parameters by subtracting from their values the derivatives of cost function taken with respect to model parameters. Properly calibrated Gradient Descent procedure should result in a process that can be visualised by the graph below - iteratively getting closer to minimal value of our cost function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent visualisation](gradient_descent_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations for updated $w$ and $b$ parameters are given below. The value of learning parameter - $\\alpha$ is choosen somewhat arbitrarily, but it cannot be too great because the training process will fail to converge - the error will increase instead of going down with each iteration. Too low $\\alpha$ on the other hand, can slow down the process unnecessarily. It is good practice to start with $\\alpha = 0.001$ and then experiment with increasing it's value 3x time or 10x each time.\n",
    "\n",
    "$$\n",
    "\\tag{6}\n",
    "\\vec{w}^{(k+1)} = \\vec{w}^{(k)} - \\alpha {\\partial J \\over \\partial \\vec{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tag{7}\n",
    "b^{(k+1)} = b^{(k)} - \\alpha {\\partial J \\over \\partial b}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "$k$ - current iteration of Gradient Descent.\n",
    "\n",
    "$k+1$ - next iteration of GD.\n",
    "\n",
    "$\\vec{w}^{(k)}$ - weights for the current iteration of GD.\n",
    "\n",
    "$\\vec{w}^{(k+1)}$ - weights for the next iteration of GD.\n",
    "\n",
    "$b^{(k)}$ - bias value for the current iteration of GD.\n",
    "\n",
    "$b^{(k+1)}$ - bias value for the next iteration of GD.\n",
    "\n",
    "$\\alpha$ - learning rate.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "To clarify how to update each weight parameter, the equation below is presented, expanding vectors into individual values. The formulae following this one will refer to individual variables.\n",
    "\n",
    "$$\n",
    "\\tag{8}\n",
    "w^{(k+1)}_i = w^{(k)}_i - \\alpha {\\partial J \\over \\partial w_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "$i$ - index of the weight parameter.\n",
    "\n",
    "$w^{(k)}_i$ - $i$-th weight value of the current GD iteration.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "The derivative of cost function $J$ with respect to $w_i$:\n",
    "\n",
    "$$\n",
    "\\tag{9}\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "{\\partial J \\over \\partial w_i} \n",
    "& = {\\partial \\over \\partial w_i} \\Big( {1 \\over 2m}\\displaystyle\\sum_{j=0}^m{(x^{(j)}_i w_i + b - y^{(j)})^2} \\Big) \\\\\n",
    "& = { 1 \\over 2m}\\displaystyle\\sum_{j=0}^m { {\\partial \\over \\partial w_i} (x^{(j)}_i w_i + b - y^{(j)})^2 } \\\\\n",
    "& = { 1 \\over 2m}\\displaystyle\\sum_{j=0}^m { 2 (x^{(j)}_i w_i + b - y^{(j)}) {\\partial \\over \\partial w_i} (x^{(j)}_i w_i + b - y^{(j)}) } \\\\\n",
    "& = { 1 \\over 2m}\\displaystyle\\sum_{j=0}^m { 2 (x^{(j)}_i w_i + b - y^{(j)}) x^{(j)}_i } \\\\\n",
    "& = { 1 \\over m}\\displaystyle\\sum_{j=0}^m { (\\hat{y}^{(j)} - y^{(j)}) x^{(j)}_i }\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "<br><br>\n",
    "And the same for bias parameter $b$:\n",
    "\n",
    "$$\n",
    "\\tag{10}\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "{\\partial J \\over \\partial b} \n",
    "& = {\\partial \\over \\partial b} \\Big( {1 \\over 2m}\\displaystyle\\sum_{j=0}^m{(x^{(j)}_i w_i + b - y^{(j)})^2} \\Big) \\\\\n",
    "& = { 1 \\over 2m}\\displaystyle\\sum_{j=0}^m { {\\partial \\over \\partial b} (x^{(j)}_i w_i + b - y^{(j)})^2 } \\\\\n",
    "& = { 1 \\over 2m}\\displaystyle\\sum_{j=0}^m { 2 (x^{(j)}_i w_i + b - y^{(j)}) {\\partial \\over \\partial b} (x^{(j)}_i w_i + b - y^{(j)}) } \\\\\n",
    "& = { 1 \\over m}\\displaystyle\\sum_{j=0}^m { (x^{(j)}_i w_i + b - y^{(j)} ) } \\\\\n",
    "& = { 1 \\over m}\\displaystyle\\sum_{j=0}^m { (\\hat{y}^{(j)} - y^{(j)}) }\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "Putting it all together, the equations for updating model parameters in each step of Gradient Descent are as follows:\n",
    "\n",
    "$$\n",
    "\\tag{11}\n",
    "w^{(k+1)}_i = w^{(k)}_i - \\alpha { 1 \\over m}\\displaystyle\\sum_{j=0}^m { (\\hat{y}^{(j)} - y^{(j)}) x_i }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tag{12}\n",
    "b^{(k+1)}_i = b^{(k)}_i - \\alpha { 1 \\over m}\\displaystyle\\sum_{j=0}^m { (\\hat{y}^{(j)} - y^{(j)}) }\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function implementation\n",
    "\n",
    "def mean_squarred_error(y_hat: np.ndarray, \n",
    "                        y: np.ndarray) -> float:\n",
    "    \"\"\"Cost function calculating loss on model output and ground truth.\n",
    "\n",
    "    Calculates cost function as mean squarred error.\n",
    "\n",
    "    Args:\n",
    "        y_hat (np.ndarray): model output vector.\n",
    "        y (np.ndarray): ground truth vector.\n",
    "\n",
    "    Returns:\n",
    "        float: cost function result - error value.\n",
    "    \"\"\"\n",
    "    assert len(y_hat) == len(y), \\\n",
    "        \"y_hat and y have to be the same length!\"\n",
    "    assert isinstance(y_hat, np.ndarray), \"y_hat must be numpy array!\"\n",
    "    assert isinstance(y, np.ndarray), \"y must be numpy array!\"\n",
    "    assert len(y.shape) == 1 or \\\n",
    "           (len(y.shape) == 2 and y.shape[1] == 1), \\\n",
    "           \"y must be one dimensional!\"\n",
    "    assert len(y_hat.shape) == 1 or \\\n",
    "           (len(y_hat.shape) == 2 and y_hat.shape[1] == 1), \\\n",
    "           \"y_hat must be one dimensional!\"\n",
    "    \n",
    "    loss = np.power(y_hat - y, 2)\n",
    "    m = len(y)\n",
    "    cost = np.sum(loss) / (m * 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_w(y_hat: np.ndarray, \n",
    "         y: np.ndarray,\n",
    "         x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate dJ/dw.\n",
    "\n",
    "    Returns numpy array of derivatives of mean squarred error cost \n",
    "    function derivatives with respect to weight parameters.\n",
    "\n",
    "    Args:\n",
    "        y_hat (np.ndarray): model output vector.\n",
    "        y (np.ndarray): ground truth vector.\n",
    "        x (np.ndarray): input variables (all inputs for all samples).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array of values of derivatives.\n",
    "    \"\"\"\n",
    "    assert len(y_hat) == len(y), \\\n",
    "        \"y_hat and y have to be the same length!\"\n",
    "    assert isinstance(y_hat, np.ndarray), \"y_hat must be numpy array!\"\n",
    "    assert isinstance(y, np.ndarray), \"y must be numpy array!\"\n",
    "    assert isinstance (x, np.ndarray), \"x must be numpy array!\"\n",
    "    assert len(y.shape) == 1 or \\\n",
    "           (len(y.shape) == 2 and y.shape[1] == 1), \\\n",
    "           \"y must be one dimensional!\"\n",
    "    assert len(y_hat.shape) == 1 or \\\n",
    "           (len(y_hat.shape) == 2 and y_hat.shape[1] == 1), \\\n",
    "           \"y_hat must be one dimensional!\"\n",
    "    assert x.shape[0] == len(y), \"x must be of shape (m, n)!\"\n",
    "    \n",
    "    m = len(y)\n",
    "\n",
    "    # reshape if needed\n",
    "    if len(y_hat.shape) == 2:\n",
    "        y_hat = y_hat.reshape((m, 1))\n",
    "    if len(y.shape) == 2:\n",
    "        y = y.reshape((m, 1))\n",
    "    \n",
    "    dJ_w_values = np.sum((y_hat - y) * x, axis=0) / m\n",
    "    return dJ_w_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_b(y_hat: np.ndarray, \n",
    "         y: np.ndarray) -> float:\n",
    "    \"\"\"Calculate dJ/db.\n",
    "\n",
    "    Returns numpy array of derivatives of mean squarred error cost \n",
    "    function derivatives with respect to bias parameter.\n",
    "\n",
    "    Args:\n",
    "        y_hat (np.ndarray): model output vector.\n",
    "        y (np.ndarray): ground truth vector.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array of values of derivatives.\n",
    "    \"\"\"\n",
    "    assert len(y_hat) == len(y), \\\n",
    "        \"y_hat and y have to be the same length!\"\n",
    "    assert isinstance(y_hat, np.ndarray), \"y_hat must be numpy array!\"\n",
    "    assert isinstance(y, np.ndarray), \"y must be numpy array!\"\n",
    "    assert isinstance (x, np.ndarray), \"x must be numpy array!\"\n",
    "    assert len(y.shape) == 1 or \\\n",
    "           (len(y.shape) == 2 and y.shape[1] == 1), \\\n",
    "           \"y must be one dimensional!\"\n",
    "    assert len(y_hat.shape) == 1 or \\\n",
    "           (len(y_hat.shape) == 2 and y_hat.shape[1] == 1), \\\n",
    "           \"y_hat must be one dimensional!\"\n",
    "    \n",
    "    m = len(y)\n",
    "\n",
    "    # reshape if needed\n",
    "    if len(y_hat.shape) == 2:\n",
    "        y_hat = y_hat.reshape((m, 1))\n",
    "    if len(y.shape) == 2:\n",
    "        y = y.reshape((m, 1))\n",
    "    \n",
    "    dJ_b_values = np.sum((y_hat - y)) / m\n",
    "    return dJ_b_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model function\n",
    "\n",
    "def linear_model_output(x: np.ndarray, \n",
    "                        w: np.ndarray, \n",
    "                        b: float) -> np.ndarray:\n",
    "    \"\"\"Calculates linear model output.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): input variables of shape (m,n).\n",
    "        w (np.ndarray): weight parameters of shape (1,n).\n",
    "        b (float): bias parameter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: linear model prediction values of shape (m,1).\n",
    "    \"\"\"\n",
    "    assert isinstance(w, np.ndarray), \"w must be numpy array!\"\n",
    "    assert isinstance(w, np.ndarray), \"w must be numpy array!\"\n",
    "    assert len(w.shape) == 2 and w.shape[1] == 1, \\\n",
    "           \"w must be numpy array of shape (n,1)!\"\n",
    "    assert len(w.shape) == 2, \\\n",
    "           \"w must be two dimensional numpy array of shape (m,n)!\"\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent implementation\n",
    "\n",
    "# Return type definition\n",
    "GradientDescentResult = dict[str, Union[np.ndarray, float]]\n",
    "\n",
    "\n",
    "def gradient_descent(y: np.ndarray, \n",
    "                     x: np.ndarray,  \n",
    "                     b: float, \n",
    "                     alpha: float,\n",
    "                     w_init: np.ndarray = None,\n",
    "                     num_epochs: int = 1000) -> GradientDescentResult:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): _description_\n",
    "        x (np.ndarray): _description_\n",
    "        b (float): _description_\n",
    "        alpha (float): _description_\n",
    "        w_init (np.ndarray, optional): _description_. Defaults to None.\n",
    "        num_epochs (int, optional): _description_. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        GradientDescentResult: _description_\n",
    "    \"\"\"\n",
    "    # initialise parameters to zero\n",
    "    input_size = x.shape[1]\n",
    "    if w_init:\n",
    "        w = w_init\n",
    "    else:\n",
    "        w = np.zeros((input_size, 1))\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        y_hat = linear_model_output(x, w, b)\n",
    "        error = mean_squarred_error(y_hat, y)\n",
    "        dJ_w_values = dJ_w(y_hat, y, x)\n",
    "        dJ_b_values = dJ_b(y_hat, y)\n",
    "        w = w - alpha * dJ_w_values\n",
    "        b = b - alpha * dJ_b_values\n",
    "    \n",
    "    return_dict = {\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"error\": error\n",
    "    }\n",
    "    return return_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75b33379528e524409b88c275a668135ad8735421655c0cae10c8443e294f644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
